\name{plot_dr}
\alias{plot_dr}
\title{
  Stacked partition plot
}
\description{
   Graphical depiction of dimension reduction as a function of the proportion of information discarded. Colors indicate counts of partitions binned into size ranges. 
}
\usage{
   plot_dr( dat, method, dist.type="p", var.vec = seq(.1, .5, length.out=10), out.file=NA, tol.missing=.2, impt=TRUE  )
}
\arguments{
  \item{dat}{
     Full dataset, NxP matrix or dataframe with N samples in rows and P variables in columns. 
}
  \item{method}{
     This argument determines the method, one of "ICC.mn", "ICC.sm", "MI.mn", "MI.sm", "minR2.mn", "minR2.sm", "PC1", or "PC1.log", used to compute the new summary variable and evaluate the proportion of information captured. See Details for descriptions of each method.
}
  \item{dist.type}{
     The agglomerative approach requires evaluating similarity between variables. There are two methods currently implemented. "p" specifies that similarity will be computed as, 1 - cor( mymat, use="pairwise.complete.obs", method="pearson" ), whereas, "s" will correspond to, 1 - cor( mymat, use="pairwise.complete.obs", method="spearman" ).
}
  \item{var.vec}{
     Vector of values specifying the information constraints, x-axis values, for reducing each candidate subset of variables into a single new summary variable. elements of var.vec must be in the interval (0, 1]. These elements specify the minimum proportions of retained information allowable in summary variables. Generally, the smaller this value, the fewer the number of variables in the reduced set.
}
  \item{out.file}{
     Name of an output file path to save pdf image.
}
  \item{tol.missing}{
     Proportion of missing data over which columns and rows will be removed from the dataset prior to generating the graph.
}
  \item{impt}{
     Logical value specifying whether missing values should be imputed by the pamr() function of the pamr R package.
}
}
\details{
  For each value of var.vec, var.vec[ j ], subsets of variables forming each partition part are formed and grown using an agglomerative approach, continuing until the proportion of information explained by the summary variable for each part drops below var.vec[ j ]. The algorithm proceeds as follows,
  \enumerate{
    \item compute a distance matrix, describing dissimilarity between all pairs of features
    \item join nearest two features into a candidate subset
    \item compute a summary variable for the candidate subset
    \item check whether pct.var of information is captured in the new summary variable 
    \item if pct.var of information is captured, then replace subset features with the summary variable
    \item update distance matrix
    \item repeat steps 2-6 until the reduced dataset converges to a stable state
  }
  In step 4, the amount of information in the new summary variable is compared against the set of original variable(s), not a summary variable.
  The above general algorithm is implemented using a number of different specific methods for evaluating proportion of information captured and computing the summary variable,
  \itemize{
    \item ICC.mn -- Intraclass correlation coefficient (ICC) is used to estimate the proportion of variance explained by the summary variable which is generated by taking the mean of values within a subset. 
    \item ICC.sm -- Same as ICC.mn, but data are transformed before the ICC computation by log(x + 1) and the summary variable is generated by summing non-transformed values within a subset. This version is designed for count data.
    \item MI.mn -- Information captured is evaluated by the standardized mutual information (MI), which is computed using the infotheo package following discretization of original variables into approximately nrow(mymat)^(1/3) bins. The summary variable is the mean.
    \item MI.sm -- Same as MI.mn, but the summary variable is generated by summing values within a subset, designed for count data.
    \item minR2.mn -- Information captured is evaluated by the minimum r-squared (Pearson correlation) between the summary variable and original variables. Summary variable is the mean.
    \item minR2.sm -- Same as minR2.mn but the summary variable is generated by summing values within a subset and Spearman correlation is used to compute r-squared instead of Pearson. This version is designed for count data.
    \item PC1 -- The summary variable is the first principal component computed from variables within the cluster only. Information captured is evaluated by the percent variation of variables within the subset explained by this summary variable.
    \item PC1.log -- Same as PC1, but data are first log transformed after adding 1, log(x + 1), designed for count data.
  }
  Each plot depicts numbers variables in the reduced sets, x-axis, following the application of the partition algorithm at specified (x-axis) levels of information loss. Colors indicate counts of partition subsets binned into size ranges. Two plots are generated. The top plot depicts results of analyses of the dataset as entered. In contrast, the bottom plot represents what the results would look like under complete independence between all variables, generated by randomly permuting values of each variable (column) with respect to all other variables.
}
\value{
  A list which includes the following columns:
  \item{ dat.plot.obs }{A dataset (dataframe) with values used to generate the upper plot.}
  \item{dat.plot.perm }{A dataset (dataframe) with values used to generate the lower plot.}
}
\references{
 Millstein J, et al.
}
\author{
  Joshua Millstein
}

\examples{

blk.vec = 2:20
c.lb = .2
c.ub = .4
n = 200

dat = sim_blk_diag_mvn( blk.vec, c.lb, c.ub, n  )

rslts = plot_dr( dat, method="PC1" )

}

\keyword{ first principal component, intraclass correlation coefficient, mutual information }


