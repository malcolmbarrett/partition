% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Partition_plotFunc_StackedArea.R
\name{plot_dr}
\alias{plot_dr}
\title{Stacked partition plot}
\usage{
plot_dr(dat, method, dist.type = "p", var.vec = seq(0.1, 0.5, length.out =
  10), out.file = NA, tol.missing = 0.2, impt = TRUE, verb = TRUE)
}
\arguments{
\item{dat}{Full dataset, NxP matrix or dataframe with N samples in rows and
P variables in columns.}

\item{method}{This argument determines the method, one of "ICC.mn",
"ICC.sm", "MI.mn", "MI.sm", "minR2.mn", "minR2.sm", "PC1", or "PC1.log",
used to compute the new summary variable and evaluate the proportion of
information captured. See Details for descriptions of each method.}

\item{dist.type}{The agglomerative approach requires evaluating similarity
between variables. There are two methods currently implemented. "p"
specifies that similarity will be computed as, 1 - cor( mymat,
use="pairwise.complete.obs", method="pearson" ), whereas, "s" will
correspond to, 1 - cor( mymat, use="pairwise.complete.obs",
method="spearman" ).}

\item{var.vec}{Vector of values specifying the information constraints,
x-axis values, for reducing each candidate subset of variables into a single
new summary variable. elements of var.vec must be in the interval (0, 1].
These elements specify the minimum proportions of retained information
allowable in summary variables. Generally, the smaller this value, the fewer
the number of variables in the reduced set.}

\item{out.file}{Name of an output file path to save pdf image.}

\item{tol.missing}{Proportion of missing data over which columns and rows
will be removed from the dataset prior to generating the graph.}

\item{impt}{Logical value specifying whether missing values should be
imputed by the pamr() function of the pamr R package.}

\item{verb}{Logical scalar. When \code{TRUE} (default) the function will print
progress messages.}
}
\value{
A list which includes the following columns:
\item{ dat.plot.obs }{A dataset (dataframe) with values used to generate the
upper plot.}
\item{dat.plot.perm }{A dataset (dataframe) with values used to generate the
lower plot.}
}
\description{
Graphical depiction of dimension reduction as a function of the proportion
of information discarded. Colors indicate counts of partitions binned into
size ranges.
}
\details{
For each value of \code{var.vec}, \code{var.vec[ j ]}, subsets of variables forming each
partition part are formed and grown using an agglomerative approach,
continuing until the proportion of information explained by the summary
variable for each part drops below \code{var.vec[ j ]}. The algorithm proceeds as
follows, \enumerate{
\item compute a distance matrix, describing dissimilarity between all pairs
of features
\item join nearest two features into a candidate subset
\item compute a summary variable for the candidate subset
\item check whether pct.var of information is captured in the new summary
variable
\item if pct.var of information is captured, then replace subset features
with the summary variable
\item update distance matrix
\item repeat steps 2-6 until the reduced dataset converges to a stable state
}

In step 4, the amount of information in the new summary variable is compared
against the set of original variable(s), not a summary variable.  The above
general algorithm is implemented using a number of different specific
methods for evaluating proportion of information captured and computing the
summary variable,

\itemize{
\item ICC.mn -- Intraclass correlation
coefficient (ICC) is used to estimate the proportion of variance explained
by the summary variable which is generated by taking the mean of values
within a subset.
\item ICC.sm -- Same as ICC.mn, but data are transformed
before the ICC computation by log(x + 1) and the summary variable is
generated by summing non-transformed values within a subset. This version is
designed for count data.
\item MI.mn -- Information captured is evaluated
by the standardized mutual information (MI), which is computed using the
infotheo package following discretization of original variables into
approximately nrow(mymat)^(1/3) bins. The summary variable is the mean.

\item MI.sm -- Same as MI.mn, but the summary variable is generated by
summing values within a subset, designed for count data.
\item minR2.mn --
Information captured is evaluated by the minimum r-squared (Pearson
correlation) between the summary variable and original variables. Summary
variable is the mean.
\item minR2.sm -- Same as minR2.mn but the summary
variable is generated by summing values within a subset and Spearman
correlation is used to compute r-squared instead of Pearson. This version is
designed for count data.
\item PC1 -- The summary variable is the first
principal component computed from variables within the cluster only.
Information captured is evaluated by the percent variation of variables
within the subset explained by this summary variable.
\item PC1.log -- Same
as PC1, but data are first log transformed after adding 1, log(x + 1),
designed for count data.
}

Each plot depicts numbers variables in the
reduced sets, x-axis, following the application of the partition algorithm
at specified (x-axis) levels of information loss. Colors indicate counts of
partition subsets binned into size ranges. Two plots are generated. The top
plot depicts results of analyses of the dataset as entered. In contrast, the
bottom plot represents what the results would look like under complete
independence between all variables, generated by randomly permuting values
of each variable (column) with respect to all other variables.
}
\examples{


blk.vec = 2:20
c.lb = .2
c.ub = .4
n = 200

dat = sim_blk_diag_mvn( blk.vec, c.lb, c.ub, n  )
\dontrun{
rslts = plot_dr( dat, method="PC1" )
}
}
\references{
Millstein J, et al.
}
\author{
Joshua Millstein
}
\keyword{coefficient,}
\keyword{component,}
\keyword{correlation}
\keyword{first}
\keyword{information}
\keyword{intraclass}
\keyword{mutual}
\keyword{principal}
