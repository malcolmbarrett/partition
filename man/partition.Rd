% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Partition_Agglomerative.R
\name{partition}
\alias{partition}
\title{Agglomerative partitioning dimension reduction}
\usage{
partition(mat, pct.var, method, dist.type = "p", niter = 1000)
}
\arguments{
\item{pct.var}{Information constraint for reducing each candidate subset of
variables into a single new summary variable. pct.var must be a value in the
interval (0, 1]. For each candidate partition (subset of the variables),
pct.var specifies the minimum proportion of retained information allowable
in the summary variable from the total information contained in all
variables within that subset. Generally, the smaller this value, the fewer
the number of variables in the reduced set. It is possible to reduce the
number of variables without discarding any information at all (pct.var = 1)
if there are multiple variables that have the same values for each
observation.}

\item{method}{This argument determines the method used to compute the new
summary variable and evaluate the proportion of information captured. See
Details for descriptions of each method.}

\item{dist.type}{The agglomerative approach requires evaluating similarity
between variables. There are two methods currently implemented. "p"
specifies that similarity will be computed as, 1 - cor( mymat,
use="pairwise.complete.obs", method="pearson" ), whereas, "s" will be, 1 -
cor( mymat, use="pairwise.complete.obs", method="spearman" ).}

\item{mymat}{Full dataset, NxP matrix or dataframe with N samples in rows
and P variables in columns.}
}
\value{
A list which includes the following columns:
\item{ mymat.r }{A dataset (dataframe) with reduced number of variables,
dimensions N x R.}
\item{clusters }{A dataset (dataframe) specifying the mapping between the
original variables and the reduced variables. Also included is a column,
pct.var, with estimates of the proportion of information of variables in a
cluster captured by the summary variable.}
}
\description{
An agglomerative partitioning strategy is used to reduce the dimensionality
of a dataset that has substantial dependencies among (at least some)
variables. An N x P dataset, with N samples and P variables is reduced to
and N x R dataset, where R < P. The amount of reduction, i.e., the dimension
of the reduced dataset, depends on a user specified information loss
constraint.
}
\details{
Clusters are formed and grown using an agglomerative approach, continuing
until the proportion of information explained by the summary variable for
each cluster drops below pct.var. The algorithm proceeds as follows,

\enumerate{
\item compute a distance matrix, describing dissimilarity between all pairs
of features
\item join nearest two features into a candidate cluster/subset
\item compute a summary variable for the candidate cluster
\item check whether pct.var of information is captured in the new summary
variable
\item if pct.var of information is captured, then replace cluster features
with the summary variable
\item update distance matrix
\item repeat steps 2-6 until the reduced dataset converges to a stable state
}

In step 4, the amount of information in the new summary variable is
compared against the set of original variable(s), not a summary variable.
The above general algorithm is implemented using a number of different
specific methods for evaluating proportion of information captured and
computing the summary variable,

\itemize{
\item ICC.mn -- Intraclass correlation coefficient (ICC) is used to
estimate the proportion of variance explained by the summary variable which
is generated by taking the mean of values within a cluster.
\item ICC.sm -- Same as ICC.mn, but data are transformed before the ICC
computation by log(x + 1) and the summary variable is generated by summing
non-transformed values within a cluster. This version is designed for count
data.
\item MI.mn -- Information
captured is evaluated by the standardized mutual information (MI), which is
computed using the infotheo package following discretization of original
variables into approximately nrow(mymat)^(1/3) bins. The summary variable is
the mean.
\item MI.sm -- Same as MI.mn, but the summary variable is generated by
summing values within a cluster, designed for count data.
\item minR2.mn -- Information captured is evaluated by the minimum r-squared
(Pearson correlation) between the summary variable and original variables.
Summary variable is the mean.
\item minR2.sm -- Same as minR2.mn but the summary variable is generated
by summing values within a cluster and Spearman correlation is used to
compute r-squared instead of Pearson. This version is designed for count
data.
\item PC1 -- The summary variable is the first principal component computed
from variables within the cluster only. Information captured is evaluated
by the percent variation of variables within the cluster explained by this
summary variable.
\item PC1.log -- Same as PC1, but data are first log transformed after
adding 1, log(x + 1), designed for count data.
}
}
\examples{


blk.vec = 2:20
c.lb = .2
c.ub = .4
n = 200

dat = sim_blk_diag_mvn( blk.vec, c.lb, c.ub, n  )

rslts = partition( dat, pct.var=.8, method="PC1", dist.type="p" )

}
\references{
Millstein J, et al.
}
\author{
Joshua Millstein
}
\keyword{coefficient,}
\keyword{component,}
\keyword{correlation}
\keyword{first}
\keyword{information}
\keyword{intraclass}
\keyword{mutual}
\keyword{principal}
