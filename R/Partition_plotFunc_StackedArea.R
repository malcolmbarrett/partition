##############################################################################################
# Program Name: Partition_plotFunc_StackedArea.R
# Purpose: Plot clustering as a function of minimum information captured.
# Programmer: Joshua Millstein
# Date: 12/01/17



#' Stacked partition plot
#' 
#' Graphical depiction of dimension reduction as a function of the proportion
#' of information discarded. Colors indicate counts of partitions binned into
#' size ranges.
#' 
#' For each value of `var.vec`, `var.vec[ j ]`, subsets of variables forming each
#' partition part are formed and grown using an agglomerative approach,
#' continuing until the proportion of information explained by the summary
#' variable for each part drops below `var.vec[ j ]`. The algorithm proceeds as
#' follows, \enumerate{ 
#'   \item compute a distance matrix, describing dissimilarity between all pairs
#'   of features 
#'   \item join nearest two features into a candidate subset 
#'   \item compute a summary variable for the candidate subset 
#'   \item check whether pct.var of information is captured in the new summary
#'   variable 
#'   \item if pct.var of information is captured, then replace subset features
#'   with the summary variable 
#'   \item update distance matrix 
#'   \item repeat steps 2-6 until the reduced dataset converges to a stable state
#' }
#' 
#' In step 4, the amount of information in the new summary variable is compared
#' against the set of original variable(s), not a summary variable.  The above
#' general algorithm is implemented using a number of different specific
#' methods for evaluating proportion of information captured and computing the
#' summary variable,
#' 
#' \itemize{ 
#'   \item ICC.mn -- Intraclass correlation
#'   coefficient (ICC) is used to estimate the proportion of variance explained
#'   by the summary variable which is generated by taking the mean of values
#'   within a subset.  
#'   \item ICC.sm -- Same as ICC.mn, but data are transformed
#'   before the ICC computation by log(x + 1) and the summary variable is
#'   generated by summing non-transformed values within a subset. This version is
#'   designed for count data.  
#'   \item MI.mn -- Information captured is evaluated
#'   by the standardized mutual information (MI), which is computed using the
#'   infotheo package following discretization of original variables into
#'   approximately nrow(mymat)^(1/3) bins. The summary variable is the mean.
#' 
#'   \item MI.sm -- Same as MI.mn, but the summary variable is generated by
#'   summing values within a subset, designed for count data.  
#'   \item minR2.mn --
#'   Information captured is evaluated by the minimum r-squared (Pearson
#'   correlation) between the summary variable and original variables. Summary
#'   variable is the mean.  
#'   \item minR2.sm -- Same as minR2.mn but the summary
#'   variable is generated by summing values within a subset and Spearman
#'   correlation is used to compute r-squared instead of Pearson. This version is
#'   designed for count data.  
#'   \item PC1 -- The summary variable is the first
#'   principal component computed from variables within the cluster only.
#'   Information captured is evaluated by the percent variation of variables
#'   within the subset explained by this summary variable.  
#'   \item PC1.log -- Same
#'   as PC1, but data are first log transformed after adding 1, log(x + 1),
#'   designed for count data.  
#' }
#' 
#' Each plot depicts numbers variables in the
#' reduced sets, x-axis, following the application of the partition algorithm
#' at specified (x-axis) levels of information loss. Colors indicate counts of
#' partition subsets binned into size ranges. Two plots are generated. The top
#' plot depicts results of analyses of the dataset as entered. In contrast, the
#' bottom plot represents what the results would look like under complete
#' independence between all variables, generated by randomly permuting values
#' of each variable (column) with respect to all other variables.
#' 
#' @param dat Full dataset, NxP matrix or dataframe with N samples in rows and
#' P variables in columns.
#' @param method This argument determines the method, one of "ICC.mn",
#' "ICC.sm", "MI.mn", "MI.sm", "minR2.mn", "minR2.sm", "PC1", or "PC1.log",
#' used to compute the new summary variable and evaluate the proportion of
#' information captured. See Details for descriptions of each method.
#' @param dist.type The agglomerative approach requires evaluating similarity
#' between variables. There are two methods currently implemented. "p"
#' specifies that similarity will be computed as, 1 - cor( mymat,
#' use="pairwise.complete.obs", method="pearson" ), whereas, "s" will
#' correspond to, 1 - cor( mymat, use="pairwise.complete.obs",
#' method="spearman" ).
#' @param var.vec Vector of values specifying the information constraints,
#' x-axis values, for reducing each candidate subset of variables into a single
#' new summary variable. elements of var.vec must be in the interval (0, 1].
#' These elements specify the minimum proportions of retained information
#' allowable in summary variables. Generally, the smaller this value, the fewer
#' the number of variables in the reduced set.
#' @param out.file Name of an output file path to save pdf image.
#' @param tol.missing Proportion of missing data over which columns and rows
#' will be removed from the dataset prior to generating the graph.
#' @param impt Logical value specifying whether missing values should be
#' imputed by the pamr() function of the pamr R package.
#' @param verb Logical scalar. When `TRUE` (default) the function will print
#' progress messages.
#' @return A list which includes the following columns:
#' \item{ dat.plot.obs }{A dataset (dataframe) with values used to generate the
#' upper plot.}
#' \item{dat.plot.perm }{A dataset (dataframe) with values used to generate the
#' lower plot.}
#' @author Joshua Millstein
#' @references Millstein J, et al.
#' @keywords first principal component, intraclass correlation coefficient,
#' mutual information
#' @examples
#' 
#' 
#' blk.vec = 2:20
#' c.lb = .2
#' c.ub = .4
#' n = 200
#' 
#' dat = sim_blk_diag_mvn( blk.vec, c.lb, c.ub, n  )
#' 
#' rslts = plot_dr( dat, method="PC1" )
#' 
#' @export
plot_dr = function(
  dat,
  method,
  dist.type   = "p",
  var.vec     = seq(.1, .5, length.out=10),
  out.file    = NA,
  tol.missing = .2,
  impt        = TRUE,
  verb        = TRUE
) {
	# pct.var = minimum variance captured in cluster
	# clst.num = number of clusters within count rng
	# clst.cnt.rng = range of number of elements within cluster, e.g., ">5"
	
	# remove rows with more than (tol.missing*100)% missing
	rind = rep(TRUE, nrow(dat))
	for(i in 1:nrow(dat)) if( sum(is.na(dat[i,])) > (tol.missing*ncol(dat)) ) rind[ i ] = FALSE
	dat = dat[ rind, ]
	
	# remove columns with more than (tol.missing*100)% missing
	cind = rep(TRUE, ncol(dat))
	for(j in 1:ncol(dat)) if( sum(is.na(dat[ , j])) > (tol.missing*nrow(dat)) ) cind[ i ] = FALSE
	dat = dat[ , cind]
	
	# impute missing with pamr
	if(impt){
		if( sum(is.na(as.numeric(as.matrix(dat)))) > .1 ){
			tmp = list(x = t(dat), y = rep(1, nrow(dat)) )
			dat = as.data.frame( t( pamr::pamr.knnimpute( tmp )$x ) )
		}
	} # end impt check
	
	c.rng = c("1", "2-4", "5-8", "9-16", "17-32", ">32")
	breaks = c(0,1,4,8,16,32,10^10)
	nms = c("pct.var.target", "pct.var.observed", "clst.num", "clst.cnt.rng" )
	dat.plot = as.data.frame( matrix(NA, nrow=length(var.vec)*length(c.rng), ncol=length(nms)) )
	names(dat.plot) = nms
	dat.plot[, "clst.cnt.rng"] = factor( rep(c.rng, length(var.vec)), levels=c.rng )
	dat.plot[, "pct.var.target"] = rep( var.vec, each=length(c.rng) )

	for( pct.var in var.vec ){
	  
	  if (verb)
		  message( sprintf( "observed %s pct.var = %04.2f",  Sys.time(), pct.var ) )
	  
		tmp = partition( dat, pct.var,  method, dist.type )
		clstr = tmp[[2]]
		dat.plot[ is.element(dat.plot$pct.var.target, pct.var), "clst.num"] = 
		  hist(table(clstr[,"cluster"]),breaks=breaks,plot=FALSE)$counts
		dat.plot[ is.element(dat.plot$pct.var.target, pct.var), "pct.var.observed"] =
		  mean( clstr[,"pct.var"], na.rm=TRUE )
		
		if (verb)
		  message(sprintf("Percent variance explained: %04.2f", pct.var))
	}

	dat.plot[, "pct.var.observed1"] = round(dat.plot[, "pct.var.observed"] * 100)
	dat.plot[, "pct.var.target1"] = round(dat.plot[, "pct.var.target"] * 100)
	
	lab.dat = dat.plot[ !duplicated( dat.plot$pct.var.target1 ), ]
	lab.dat = lab.dat[ order( lab.dat$pct.var.target1 ), c("pct.var.target1", "pct.var.observed1") ]
	mypos = c( min(dat.plot[, "pct.var.target1"]), max(dat.plot[, "pct.var.target1"]), 
					mean( c(min(dat.plot[, "pct.var.target1"]), max(dat.plot[, "pct.var.target1"])) ) )
	
	pct.var.target1 <- clst.num <- clst.cnt.rng <- NULL
	p1 = ggplot2::ggplot(dat.plot, aes(x=pct.var.target1, y=clst.num, fill=clst.cnt.rng)) +
	  ggplot2::theme_bw() +
		ggplot2::geom_area(colour="black", size=.2, alpha=.4) +
	  ggplot2::scale_fill_brewer(palette="Set1") +
	  ggplot2::scale_x_continuous(
	    limits   = mypos[1:2], 
	    breaks   =lab.dat$pct.var.target1,
	    sec.axis = sec_axis(~ ., breaks=lab.dat$pct.var.target1, labels=lab.dat$pct.var.observed1)
	    ) +
	  ggplot2::xlab("minimum variance explained (%)") +
	  ggplot2::ylab("k") +
	  ggplot2::labs(title="Observed") +
	  ggplot2::theme(legend.position = c(0.1, .65)) +
	  ggplot2::guides(fill=guide_legend(title="Partition Size")) +
	  ggplot2::annotate("text", x=mypos[3], y=1.05*nrow(clstr), label="variance captured (%)")
    
	dat.plot.obs = dat.plot    
    
	# Recreate graph after randomizing data
	dat1 = dat
	dat.plot1 = dat.plot
	for(j in 1:ncol(dat1)) dat1[,j] = dat1[ sample(1:nrow(dat1), nrow(dat1)), j]

	for( pct.var in var.vec ){
	  
	  if (verb)
	    message( sprintf( "permuted %s pct.var=%04.2f",  Sys.time(), pct.var ) )
	  
		tmp = partition( dat, pct.var,  method, dist.type )
		clstr = tmp[[2]]
		dat.plot[ is.element(dat.plot$pct.var.target, pct.var), "clst.num"] =
		  hist(table(clstr[,"cluster"]),breaks=breaks,plot=FALSE)$counts
		dat.plot[ is.element(dat.plot$pct.var.target, pct.var), "pct.var.observed"] =
		  mean( clstr[,"pct.var"], na.rm=TRUE )
		
		if (verb)
		message(sprintf("Percent variance explained: %04.2f", pct.var))
	}

	dat.plot[, "pct.var.observed1"] = round(dat.plot[, "pct.var.observed"] * 100)
	dat.plot[, "pct.var.target1"] = round(dat.plot[, "pct.var.target"] * 100)
	
	lab.dat = dat.plot[ !duplicated( dat.plot$pct.var.target1 ), ]
	lab.dat = lab.dat[ order( lab.dat$pct.var.target1 ), c("pct.var.target1", "pct.var.observed1") ]
	mypos = c( min(dat.plot[, "pct.var.target1"]), max(dat.plot[, "pct.var.target1"]), 
					mean( c(min(dat.plot[, "pct.var.target1"]), max(dat.plot[, "pct.var.target1"])) ) )

	p2 = ggplot2::ggplot(dat.plot, aes(x=pct.var.target1, y=clst.num, fill=clst.cnt.rng)) + 
	  ggplot2::theme_bw() +
		ggplot2::geom_area(colour="black", size=.2, alpha=.4) +
		ggplot2::scale_fill_brewer(palette="Set1") +
	  ggplot2::scale_x_continuous(
	    limits = mypos[1:2], breaks=lab.dat$pct.var.target1,
	    sec.axis = sec_axis(~ ., breaks=lab.dat$pct.var.target1, labels=lab.dat$pct.var.observed1)
	    ) +
		ggplot2::xlab("minimum variance explained (%)") +
	  ggplot2::ylab("k") +
	  ggplot2::labs(title="Randomized") +
		ggplot2::theme(legend.position = c(0.1, .65)) +
		ggplot2::guides(fill=guide_legend(title="Cluster Size")) +
		ggplot2::annotate(
		  "text", x=mypos[3], y=1.05*nrow(clstr),
		  label="variance captured (%)")
	
	dat.plot.perm = dat.plot 

	if( is.na(out.file) ){
		gridExtra::grid.arrange( p1, p2, ncol=1)
	} else {
		pdf( out.file )
			gridExtra::grid.arrange( p1, p2, ncol=1)
		dev.off()
	}
	
	outp = vector('list', 2)
	outp[[ 1 ]] = dat.plot.obs
	outp[[ 2 ]] = dat.plot.perm
	return( outp )
	
} # End plot_dr



